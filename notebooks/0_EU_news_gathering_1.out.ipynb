{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Extraction code"
   ],
   "id": "48e032a6-fcc3-4511-b640-4d871ec15b74"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [],
   "id": "cell-1"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "from dotenv import load_dotenv"
   ],
   "id": "cell-2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading API Key"
   ],
   "id": "cell-3"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading API KEY from environment\n",
    "load_dotenv()\n",
    "newscatcher_key_v2 = os.getenv(\"newscatcher_key_v2\")\n",
    "newscatcher_key_v3 = os.getenv(\"newscatcher_key_v3\")"
   ],
   "id": "cell-4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Subscription Plan (only for V3)"
   ],
   "id": "cell-5"
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'active': True, 'concurrent_calls': 1, 'plan': 'v3_nlp', 'plan_calls': 10000, 'remaining_calls': 7557, 'historical_days': 60}"
     ]
    }
   ],
   "source": [
    "# Defining URL, header, and parameters\n",
    "url      = \"https://v3-api.newscatcherapi.com/api/subscription\"\n",
    "headers  = {\"x-api-token\" : newscatcher_key_v3}\n",
    "\n",
    "# Sending the GET\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Parsening results\n",
    "json_format = json.loads(response.text)\n",
    "print(json_format)"
   ],
   "id": "cell-6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Keywords and Sources"
   ],
   "id": "cell-7"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = pd.read_excel(\"../inputs/EU_sources_1.xlsx\")\n",
    "sources[\"shortURL\"] = sources[\"URL\"].replace(r\"^https?://|www\\.|/\", \"\", regex=True)"
   ],
   "id": "cell-8"
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords      = pd.read_excel(\"../inputs/keywords_1.xlsx\")\n",
    "keywords_long = pd.melt(\n",
    "    keywords, \n",
    "    id_vars    = \"Group\", \n",
    "    var_name   = \"language\", \n",
    "    value_name = \"keyword\"\n",
    ")"
   ],
   "id": "cell-9"
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "institutions = pd.read_excel(\"../inputs/country_institutions_1.xlsx\").dropna(subset=[\"translation\"])"
   ],
   "id": "cell-10"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering News Articles"
   ],
   "id": "cell-11"
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gatherKeywords(language, country):\n",
    "    \"\"\"\n",
    "    This function takes a language and a country as arguments and retrieve a set of query-styled strings\n",
    "    from the keywords and institutions data frames.\n",
    "    \"\"\"\n",
    "\n",
    "    output = []\n",
    "\n",
    "    # Collapsing keyword batches\n",
    "    batches = sorted(keywords_long[\"Group\"].unique())\n",
    "    for batch in batches:\n",
    "        subset  = (\n",
    "            keywords_long\n",
    "            .copy()\n",
    "            .loc[keywords_long[\"language\"] == language]\n",
    "            .loc[keywords_long[\"Group\"] == batch]\n",
    "        )\n",
    "        query_style = \" OR \".join(['\"' + word + '\"' for word in subset.keyword])\n",
    "        query_style = query_style.replace(\"/\", '\" OR \"')\n",
    "        output.append(query_style)\n",
    "    \n",
    "    # Collapsing institutional names\n",
    "    institutional_names = (\n",
    "        institutions\n",
    "        .copy()\n",
    "        .loc[institutions[\"country\"] == country]\n",
    "    )\n",
    "    query_style = \" OR \".join(['\"' + word + '\"' for word in institutional_names.translation])\n",
    "    output.append(query_style)\n",
    "\n",
    "    return output\n",
    "\n",
    "def newsFetcher(query, source, date_0 = \"7 months ago\", date_1 = \"2 months ago\", v2 = True):\n",
    "    \"\"\"\n",
    "    This function takes a query and a news source as inputs and returns a data frame with all the results of that specific query \n",
    "    through either V2 or V3 of the Newscatcher API version.\n",
    "    \"\"\"\n",
    "\n",
    "    # Defining initial counters\n",
    "    page   = 1\n",
    "    npage  = 100\n",
    "\n",
    "    # Creating an empty list to store results\n",
    "    outputs = []\n",
    "\n",
    "    while page <= npage:\n",
    "\n",
    "        # Defining URL, header, and parameters\n",
    "        if v2 == True:\n",
    "            url      = \"https://api.newscatcherapi.com/v2/search?\"\n",
    "            headers  = {\"x-api-key\" : newscatcher_key_v2}\n",
    "            params   = {\n",
    "                \"q\"         : query,\n",
    "                \"sources\"   : source,\n",
    "                \"page\"      : page,\n",
    "                \"page_size\" : 100,\n",
    "                \"from\"      : date_0,\n",
    "                \"to\"        : date_1,\n",
    "                \"sort_by\"   : \"date\"\n",
    "            }\n",
    "        else:\n",
    "            url      = \"https://v3-api.newscatcherapi.com/api/search?\"\n",
    "            headers  = {\"x-api-token\" : newscatcher_key_v3}\n",
    "            params   = {\n",
    "                \"q\"         : query,\n",
    "                \"sources\"   : source,\n",
    "                \"page\"      : page,\n",
    "                \"page_size\" : 1000,\n",
    "                \"from_\"     : date_0,\n",
    "                \"to_\"       : date_1,\n",
    "                \"sort_by\"   : \"date\"\n",
    "            }\n",
    "    \n",
    "        # Sending a GET call\n",
    "        response = requests.get(url, params=params, headers=headers)\n",
    "        time.sleep(1) # The API has a restriction of 1 call per second\n",
    "\n",
    "        # Parsening the response in a JSON format\n",
    "        json_data = json.loads(response.text)\n",
    "\n",
    "        # Updating counters\n",
    "        npage      = json_data[\"total_pages\"]\n",
    "        total_hits = json_data[\"total_hits\"]\n",
    "\n",
    "        if total_hits > 0:\n",
    "        \n",
    "            # Converting results to pandas data frame\n",
    "            df = pd.DataFrame(json_data[\"articles\"])\n",
    "            min_date   = df.iloc[-1][\"published_date\"]\n",
    "            outputs.append(df)\n",
    "\n",
    "            # Increasing/Reseting counts\n",
    "            if total_hits < 10000 or page < npage:\n",
    "                page = page + 1\n",
    "            else:\n",
    "                page   = 1\n",
    "                date_1 = min_date \n",
    "    \n",
    "    # Merging entire list of data frames\n",
    "    if outputs:\n",
    "        results = pd.concat(outputs, ignore_index=True)\n",
    "    else:\n",
    "        results = None\n",
    "\n",
    "    return results\n",
    "\n",
    "def extractNews(source, country, language, from_ = \"7 months ago\", to_ = \"2 months ago\", v2 = True):\n",
    "    \"\"\"\n",
    "    This functions takes a source's URL, the country it belongs to, and the language of the publication and \n",
    "    it retrieves news articles associated to the pre-defined queries within that news source.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Extracting news articles from: {source}\")\n",
    "    \n",
    "    # Creating an empty dictionary to store results\n",
    "    results_per_batch = {}\n",
    "\n",
    "    # Iterating across batches of keywords/queries\n",
    "    qbatches = gatherKeywords(language, country)\n",
    "    for n, batch in enumerate(qbatches, start = 1):\n",
    "        print(f\"===== Extracting articles from Batch #{n}\")\n",
    "        batch_name   = f\"Batch {n}\"\n",
    "        fetched_news = newsFetcher(batch, source, from_, to_, v2)\n",
    "        dict = {\n",
    "            batch_name : fetched_news\n",
    "        }\n",
    "        results_per_batch.update(dict)\n",
    "\n",
    "    # Defining the outcome\n",
    "    output = {\n",
    "        source : results_per_batch\n",
    "    }\n",
    "\n",
    "    return output\n",
    "\n",
    "def mergeData(dta, version):\n",
    "    \"\"\"\n",
    "    This function takes a list containing all the returned data from the API and compiles it into a data set\n",
    "    \"\"\"\n",
    "    for element in dta:\n",
    "        for source, batches in element.items():\n",
    "            print(source)\n",
    "            data_list    = [data for batch, data in batches.items()]\n",
    "            empty_source = all(data is None for data in data_list)\n",
    "            if not empty_source:\n",
    "                master_data  = pd.concat(data_list)\n",
    "                master_data.to_parquet(f\"../data/data-extraction-1/{version}/{source}.parquet.gzip\", compression = \"gzip\")    "
   ],
   "id": "cell-12"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using API V2 to gather news"
   ],
   "id": "cell-13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list_v2 = (\n",
    "    sources\n",
    "    # .loc[64:64]\n",
    "    .apply(lambda row: extractNews(row[\"shortURL\"], row[\"Country\"], row[\"Language\"]), axis = 1)\n",
    "    .tolist()\n",
    ")"
   ],
   "id": "cell-14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergeData(results_list_v2, version = \"v2\")"
   ],
   "id": "cell-15"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using API V3 to gather news"
   ],
   "id": "cell-16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list_v3 = (\n",
    "    sources\n",
    "    # .loc[(sources[\"Priority\"] == \"Yes\") & (sources[\"HP\"] == \"No\")]\n",
    "    .loc[sources[\"Priority\"] == \"No\"]\n",
    "    .apply(lambda row: extractNews(\n",
    "        row[\"shortURL\"], row[\"Country\"], row[\"Language\"],\n",
    "        from_ = \"2 months ago\", to_ = \"1 day ago\", v2 = False\n",
    "    ), axis = 1)\n",
    "    .tolist()\n",
    ")"
   ],
   "id": "cell-17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergeData(results_list_v3, version = \"v3\")"
   ],
   "id": "cell-18"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling and saving data"
   ],
   "id": "cell-19"
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_v2 = os.listdir(\"../data/data-extraction-1/v2\")\n",
    "data_sources_v2 = [\n",
    "    pd.read_parquet(f\"../data/data-extraction-1/v2/{x}\")\n",
    "    for x in files_v2\n",
    "]\n",
    "master_v2 = pd.concat(data_sources_v2).drop_duplicates(subset = \"_id\")\n",
    "master_v2.to_parquet(\"../data/data-extraction-1/master_1v2.parquet.gzip\", compression = \"gzip\")"
   ],
   "id": "cell-20"
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_v3 = os.listdir(\"../data/data-extraction-1/v3\")\n",
    "data_sources_v3 = [\n",
    "    pd.read_parquet(f\"../data/data-extraction-1/v3/{x}\")\n",
    "    for x in files_v3\n",
    "]\n",
    "master_v3 = pd.concat(data_sources_v3).drop_duplicates(subset = \"id\")\n",
    "master_v3.to_parquet(\"../data/data-extraction-1/master_1v3.parquet.gzip\", compression = \"gzip\")"
   ],
   "id": "cell-21"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Harmonizing V2 and V3 data"
   ],
   "id": "cell-22"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_v2 = pd.read_parquet(\"../data/data-extraction-1/master_1v2.parquet.gzip\")\n",
    "master_v3 = pd.read_parquet(\"../data/data-extraction-1/master_1v3.parquet.gzip\")"
   ],
   "id": "cell-23"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_columns = [\"id\", \"link\", \"domain_url\", \"published_date\", \"title\", \"description\", \"content\", \"language\", \"is_opinion\"]\n",
    "master_v2_harm = (\n",
    "    master_v2\n",
    "    .rename(\n",
    "        columns = {\n",
    "            \"excerpt\"   : \"description\",\n",
    "            \"summary\"   : \"content\",\n",
    "            \"clean_url\" : \"domain_url\",\n",
    "            \"_id\"       : \"id\"\n",
    "        }\n",
    "    )\n",
    "    .loc[:, target_columns]\n",
    ")\n",
    "master_v3_harm = master_v3.loc[:, target_columns]\n",
    "master_data    = (\n",
    "    pd.concat([master_v2_harm, master_v3_harm])\n",
    "    .drop_duplicates()\n",
    ")\n",
    "master_data = master_data.loc[master_data[[\"title\", \"content\"]].notna().all(axis=1)]"
   ],
   "id": "cell-24"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving data"
   ],
   "id": "cell-25"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_data.to_parquet(\"../data/data-extraction-1/master_extraction_1.parquet.gzip\", compression = \"gzip\")"
   ],
   "id": "cell-26"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsetting data for translation"
   ],
   "id": "cell-27"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_data = pd.read_parquet(\"../data/data-extraction-1/master_extraction_1.parquet.gzip\")"
   ],
   "id": "cell-28"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_journals = {\n",
    "      \"Luxembourg\"  : ['lequotidien.lu','tageblatt.lu','wort.lu'],\n",
    "      \"Slovakia\"    : ['dennikn.sk','sme.sk', 'pravda.sk'],\n",
    "      \"Hungary\"     : ['magyarhirlap.hu','magyarnemzet.hu','nepszava.hu'],\n",
    "      \"Ireland\"     : ['irishexaminer.com','independent.ie','irishtimes.com'],\n",
    "      \"Bulgaria\"    : ['24chasa.bg','capital.bg','dnevnik.bg'],\n",
    "      \"Greece\"      : ['kathimerini.gr','protothema.gr','tanea.gr'],\n",
    "      \"Portugal\"    : ['expresso.pt','publico.pt','jn.pt'],\n",
    "      \"Cyprus\"      : ['philenews.com','politis.com.cy','sigmalive.com'],\n",
    "      \"Czechia\"     : ['blesk.cz','denik.cz','lidovky.cz'],\n",
    "      \"Estonia\"     : ['delfi.ee','postimees.ee'],\n",
    "      \"Finland\"     : ['aamulehti.fi','hs.fi','ksml.fi'],\n",
    "      \"France\"      : ['liberation.fr','lefigaro.fr','lemonde.fr'],\n",
    "      \"Slovenia\"    : ['delo.si','dnevnik.si','vecer.com'],\n",
    "      \"Spain\"       : ['abc.es','elmundo.es','elpais.com','lavanguardia.com'],\n",
    "      \"Sweden\"      : ['dn.se','gp.se','svd.se'],\n",
    "      \"Austria\"     : ['derstandard.at','diepresse.com','sn.at'],\n",
    "      \"Belgium\"     : ['hln.be','lesoir.be','standaard.be'],\n",
    "      \"Denmark\"     : ['berlingske.dk','jyllands-posten.dk','politiken.dk'],\n",
    "      \"Germany\"     : ['faz.net','spiegel.de','sueddeutsche.de','zeit.de'],\n",
    "      \"Italy\"       : ['corriere.it','lastampa.it','repubblica.it'],\n",
    "      \"Netherlands\" : ['ad.nl','nrc.nl','telegraaf.nl','volkskrant.nl'],\n",
    "      \"Croatia\"     : ['jutarnji.hr','novilist.hr','vecernji.hr'],\n",
    "      \"Malta\"       : ['independent.com.mt','timesofmalta.com','talk.mt'],\n",
    "      \"Latvia\"      : ['db.lv','ir.lv','la.lv'],\n",
    "      \"Lithuania\"   : ['baltic-review.com','lrytas.lt','ve.lt'],\n",
    "      \"Poland\"      : ['fakt.pl','rp.pl','wyborcza.pl'],\n",
    "      \"Romania\"     : ['adevarul.ro','evz.ro','libertatea.ro']\n",
    "}\n",
    "\n",
    "def assign_country(domain_url):\n",
    "    for country, urls in top_journals.items():\n",
    "        if domain_url in urls:\n",
    "            return country\n",
    "    return \"Low priority journal\"\n",
    "\n",
    "master_data[\"country\"] = master_data[\"domain_url\"].apply(assign_country)"
   ],
   "id": "cell-29"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Austria: 46145\n",
      "Belgium: 21287\n",
      "Bulgaria: 38118\n",
      "Croatia: 37068\n",
      "Cyprus: 34955\n",
      "Czechia: 41415\n",
      "Denmark: 11312\n",
      "Estonia: 12370\n",
      "Finland: 6647\n",
      "France: 64527\n",
      "Germany: 45321\n",
      "Greece: 49504\n",
      "Hungary: 18111\n",
      "Ireland: 48409\n",
      "Italy: 93858\n",
      "Latvia: 5487\n",
      "Lithuania: 14396\n",
      "Luxembourg: 7894\n",
      "Malta: 10842\n",
      "Netherlands: 23935\n",
      "Poland: 21434\n",
      "Portugal: 29624\n",
      "Romania: 33264\n",
      "Slovakia: 34874\n",
      "Slovenia: 10211\n",
      "Spain: 112820\n",
      "Sweden: 6417"
     ]
    }
   ],
   "source": [
    "for country in sources[\"Country\"].drop_duplicates().to_list():\n",
    "    subset4translation = (\n",
    "        master_data.copy()\n",
    "        .loc[(master_data[\"country\"] == country) & (master_data[\"is_opinion\"] == False)]\n",
    "    )\n",
    "    print(f\"{country}: {len(subset4translation)}\")\n",
    "    file_name = f\"../data/data-extraction-1/data4translation/{country}_tp.parquet.gzip\"\n",
    "    subset4translation.to_parquet(file_name, compression = \"gzip\")"
   ],
   "id": "cell-30"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
